In Hadoop, MapReduce is a computation that decomposes large manipulation jobs into individual tasks that can be executed in parallel cross a cluster of servers. The results of tasks can be joined together to compute final results.MapReduce works by breaking the processing into two phases: the map phase and the reduce phase. Each phase has key-value pairs as input and output, the types of which may be chosen by the programmer.
MapReduce is the core component of Hadoop that process huge amount of data in parallel by dividing the work into a set of independent tasks. In MapReduce data flow in step by step from Mapper to Reducer.

Mapper:
It processes each input record (from RecordReader) and generates new key-value pair, and this key-value pair generated by Mapper is completely different from the input pair. The output of Mapper is also known as intermediate output which is written to the local disk. The output of the Mapper is not stored on HDFS as this is temporary data and writing on HDFS will create unnecessary copies (also HDFS is a high latency system). Mappers output is passed to the combiner for further process
Shuffling and Sorting
Now, the output is Shuffled to the reduce node (which is a normal slave node but reduce phase will run here hence called as reducer node). The shuffling is the physical movement of the data which is done over the network. Once all the mappers are finished and their output is shuffled on the reducer nodes, then this intermediate output is merged and sorted, which is then provided as input to reduce phase.
Reducer
It takes the set of intermediate key-value pairs produced by the mappers as the input and then runs a reducer function on each of them to generate the output. The output of the reducer is the final output, which is stored in HDFS. Follow this link to learn about Reducer in detail.

------------------------------------
#################
words.txt
#################
Bus, Car, bus,  car, train, car, bus, car, train, bus, TRAIN,BUS, buS, caR, CAR, car, BUS, TRAIN


Steps for execution in python:
Step 1: Open terminal and create a text file and move it to the hadoop store using the following command.
Make a directory
$ hadoop fs -mkdir /wordcount
$hadoop fs -mkdir /wordput/inp
Move the text file onto the hadoop store.
$ hadoop fs -put /home /Desktop/Lab/word.txt

Step 2: Type in the mapper code and reducer code.
#################
my_mapper.py
#################
#!/usr/bin/env python2
"""wc_mapper.py"""

import sys;

for line in sys.stdin:
    line = line.strip()
    words = line.split()
    for word in words:
        print('%s\t%s' %(word, 1))

#################
my_reducer.py
#################
#!/usr/bin/env python2
"""wc_reducer.py"""

import sys

current_word = None
current_count = 0
word = None

for line in sys.stdin:
    line = line.strip()
    # parse the input we got from mapper.py
    word, count = line.split('\t', 1)
    count = int(count)
    # this IF-switch only works because Hadoop sorts map output
    if current_word == word:
        current_count += count
    else:
        if current_word:
            print('%s\t%s' %(current_word, current_count))
        current_count = count
        current_word = word
        
#output the last word if needed!
if current_word == word:
    print('%s\t%s' %(current_word, current_count))

Step 3: Run the following command.
$ hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.9.0.jar -file
/home/Desktop/Lab/word.txt	-mapper	“python3	mapper.py”	-file	/	home
/Desktop/Lab/reducer.py -reducer “python3 reducer.py” -input /wordcount/inp	-output
/wordcount/out
Step 4: Open result on the terminal Or you can see your result in Hadoop Web Interface http://localhost:50070/
Goto utilities> Browse file System> /wordcount/out
 
